{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADM Homework - 3\n",
    "#### Goal of the homework: Build a search engine over the \"best books ever\" list of GoodReads. Unless differently specified, all the functions must be implemented from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data collection\n",
    "*For this homework, there is no provided dataset, but you have to build our own. Our search engine will run on text documents.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version:  3.8.5\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(\"Python Version: \",python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Get the list of books\n",
    "*We start from the list of books to include in your corpus of documents. In particular, we focus on the best books ever list. From this list we want to collect the url associated to each book in the list. As you realize, the list is long and splitted in many pages. We ask you to retrieve only the urls of the books listed in the **First 300 pages**.*\n",
    "\n",
    "*The output of this step is a **.txt** file whose single line corresponds to a **Book's URL**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [C:\\Users\\Zain\\.wdm\\drivers\\geckodriver\\win64\\v0.28.0\\geckodriver.exe] found in cache\n",
      "[WDM] - Driver [C:\\Users\\Zain\\.wdm\\drivers\\geckodriver\\win64\\v0.28.0\\geckodriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from bs4 import BeautifulSoup as bsp\n",
    "from selenium import webdriver #Programmatic way to use Browser\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from functions import findUrls, findBookUrls,carwlBooks,remove_tags,Parse_Web_Pages,Parse_Data,executeQuery,read_data,find_set,clean_info1\n",
    "driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl.download('stopwords')\n",
    "nl.download('wordnet')\n",
    "#Below given \"href\" is the Start page for the the Target link the get books URL's from all pages.\n",
    "\n",
    "#In the funciton \"findBooksUrls\" will dynamically change the page for Upcomming Pages \n",
    "href = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1\"\n",
    "\n",
    "Urls_Dic = findBookUrls(href,driver,300)\n",
    "\n",
    "# Given Below is the file path to save the final dictionary to the given PATH.\n",
    "DictionaryPKL = open(r\"C:\\Users\\Zain\\Desktop\\ROMA\\Sapienza\\ADM\\HW3\\300PagesDic.pkl\",\"wb\")\n",
    "pickle.dump(Urls_Dic,DictionaryPKL)\n",
    "DictionaryPKL.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\Zain\\Desktop\\ROMA\\Sapienza\\ADM\\HW3\\300PagesDic.pkl', 'rb') as handle:\n",
    "    pagesDict = pickle.load(handle)\n",
    "\n",
    "with open(r'C:\\Users\\Zain\\Desktop\\ROMA\\Sapienza\\ADM\\HW3\\300Pages_UrlsList.txt','w+') as File:\n",
    "    FlatList = [ item for elem in list(pagesDict.values()) for item in elem]\n",
    "    #File.write('\\n'.join(FlatList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/book/show/2767052-the-hunger-games',\n",
       " '/book/show/2.Harry_Potter_and_the_Order_of_the_Phoenix',\n",
       " '/book/show/2657.To_Kill_a_Mockingbird',\n",
       " '/book/show/1885.Pride_and_Prejudice',\n",
       " '/book/show/41865.Twilight',\n",
       " '/book/show/19063.The_Book_Thief',\n",
       " '/book/show/170448.Animal_Farm',\n",
       " '/book/show/11127.The_Chronicles_of_Narnia',\n",
       " '/book/show/30.J_R_R_Tolkien_4_Book_Boxed_Set',\n",
       " '/book/show/11870085-the-fault-in-our-stars']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now as we have got all the links to the indivisual books from all pages\n",
    "# Given below is the example how the Data is stored in the dictionary\n",
    "# Dictionary ==> pagesDict[key,values] where [key=page No, values=Books Urls] \n",
    "pagesDict['page1'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### 1.2. Crawl books\n",
    "*Once you get all the urls in the first 300 pages of the list, you:*<br>\n",
    "<br>\n",
    "*Download the html corresponding to each of the collected urls.*<br>\n",
    "**After you collect a single page, immediatly save its html in a file. In this way, if your program stops, for any reason, you will not loose the data collected up to the stopping point. More details in Important (2).**<br>\n",
    "Organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the books in page 1, page 2, ... of the list of books.<br><br>\n",
    "**Important!!<br><br>\n",
    "Due to the large amount of pages you need to download, we give you the following tipds that help you to speed up some time-consuming operations.**<br>\n",
    "<br>\n",
    "**[Save time downloading files]** You are asked to crawl tons of pages, and this will take a lot of time. To speed up the operation, we suggest you to work in parallel with your group's colleagues. In particular, using the same code, each component of the group can be in charge of downloading a subset of pages (e.g., the first 100). PAY ATTENTION: Once obtained all the pages, merge your results in a unique dataset. In fact, the search engine must look up for results in the whole set of documents.\n",
    "<br><br>\n",
    "**[Save your data]** It is not nice to restart a crawling procedure, given its runtime. For this reason, it is extremely important that for every time you crawl a page, you must save it with the name article_i.html, where i corresponds to the number of articles you have already downloaded. In such way, if something goes bad, you can restart your crawling procedure from the i+1-th document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page1==> [File:0 Downloaded]\n",
      "page1==> [File:1 Downloaded]\n",
      "page1==> [File:2 Downloaded]\n"
     ]
    }
   ],
   "source": [
    "#Enter Start and End Range of Pages\n",
    "#This will take a range of pages and will download it in the local directory\n",
    "Start = 1\n",
    "End = 1\n",
    "\n",
    "#Change the Path here\n",
    "FilePath = r\"Users\\Zain\\Desktop\\ROMA\\Sapienza\\ADM\\HW3\"\n",
    "\n",
    "#Crawlbooks funciton Crawl all links and downlaod the HTML paegs to local directory \n",
    "carwlBooks(FilePath,Start,End)\n",
    "\n",
    "#Given below OUTPUT is just a sample for downloading 3 HTML files to the FilePath given above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### 1.3 Parse downloaded pages\n",
    "At this point, you should have all the html documents about the books of interest and you can start to extract the books informations. The list of information we desire for each book are the following:\n",
    "<br><br>\n",
    "- Title (to save as bookTitle)\n",
    "- Series (to save as bookSeries)\n",
    "- Author(s), the first box in the picture below (to save as bookAuthors)\n",
    "- Ratings, average stars (to save as ratingValue)\n",
    "- Number of givent ratings (to save as ratingCount)\n",
    "- Number of reviews (to save as reviewCount)\n",
    "- The entire plot (to save as Plot)\n",
    "- Number of pages (to save as NumberofPages)\n",
    "- Published (Publishing Date)\n",
    "- Characters\n",
    "- Setting\n",
    "- Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Start = 0 #Start point of your pages\n",
    "End = 0 #End point of your downloaded webpages\n",
    "File_path = r\"C:\\Users\\Zain\\Desktop\\ROMA\\Sapienza\\ADM\\HW3\"\n",
    "\n",
    "#Parse_Data will retrieve all the Data from the downloaded pages and will save it as a TSV file\n",
    "Parse_Data(File_path,Start,End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>bookSeries</th>\n",
       "      <th>bookAuthors</th>\n",
       "      <th>ratingValue</th>\n",
       "      <th>ratingCount</th>\n",
       "      <th>reviewCount</th>\n",
       "      <th>Plot_Values</th>\n",
       "      <th>Plot</th>\n",
       "      <th>NumberofPages</th>\n",
       "      <th>Publishing_Date</th>\n",
       "      <th>Characters</th>\n",
       "      <th>Setting</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MARS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jasmine  Rose</td>\n",
       "      <td>4.38</td>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>{5: '47', 4: '7', 3: '11', 2: '2', 1: '2'}</td>\n",
       "      <td>❝�� my heart has become a planetand you are th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.goodreads.com/book/show/23279048-mars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Black Box</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cassia Leo</td>\n",
       "      <td>4.02</td>\n",
       "      <td>6244</td>\n",
       "      <td>903</td>\n",
       "      <td>{5: '2297', 4: '2320', 3: '1181', 2: '345', 1:...</td>\n",
       "      <td>♥️ Three fateful encounters....♥️ Two heart-br...</td>\n",
       "      <td>400.0</td>\n",
       "      <td>February 28th 2014</td>\n",
       "      <td>['Mikki Gladstone', 'William \"Crush\" Slayer']</td>\n",
       "      <td>['Boston, Massachusetts']</td>\n",
       "      <td>https://www.goodreads.com/book/show/29539518-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Ruin and Rising</td>\n",
       "      <td>The Shadow and Bone Trilogy #3</td>\n",
       "      <td>Leigh Bardugo</td>\n",
       "      <td>4.09</td>\n",
       "      <td>158624</td>\n",
       "      <td>19396</td>\n",
       "      <td>{5: '62107', 4: '59607', 3: '27962', 2: '6810'...</td>\n",
       "      <td>▶ \\nAlternative Cover Edition #1\\nThe capital ...</td>\n",
       "      <td>422.0</td>\n",
       "      <td>June 17th 2014</td>\n",
       "      <td>['Alina Starkov', 'Malyen Oretsev', 'Darkling'...</td>\n",
       "      <td>['Ravka ']</td>\n",
       "      <td>https://www.goodreads.com/book/show/14061957-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Shadow and Bone</td>\n",
       "      <td>The Shadow and Bone Trilogy #1</td>\n",
       "      <td>Leigh Bardugo</td>\n",
       "      <td>3.99</td>\n",
       "      <td>313190</td>\n",
       "      <td>32760</td>\n",
       "      <td>{5: '108466', 4: '121286', 3: '62748', 2: '146...</td>\n",
       "      <td>▶ \\nAlternative Cover Edition #1\\nSurrounded b...</td>\n",
       "      <td>358.0</td>\n",
       "      <td>June 5th 2012</td>\n",
       "      <td>['Alina Starkov', 'Malyen Oretsev', 'Darkling'...</td>\n",
       "      <td>['Ravka ']</td>\n",
       "      <td>https://www.goodreads.com/book/show/10194157-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>An Ember in the Ashes</td>\n",
       "      <td>An Ember in the Ashes #1</td>\n",
       "      <td>Sabaa Tahir</td>\n",
       "      <td>4.27</td>\n",
       "      <td>184959</td>\n",
       "      <td>23147</td>\n",
       "      <td>{5: '91602', 4: '63735', 3: '21164', 2: '5487'...</td>\n",
       "      <td>▶ \\nAlternative Cover Edition #1\\nLaia is a sl...</td>\n",
       "      <td>446.0</td>\n",
       "      <td>February 9th 2016</td>\n",
       "      <td>['Laia', 'Elias Veturius', 'Helene Aquilla', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.goodreads.com/book/show/27774758-a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              bookTitle                      bookSeries  \\\n",
       "0           0                   MARS                             NaN   \n",
       "1           1              Black Box                             NaN   \n",
       "2           2        Ruin and Rising  The Shadow and Bone Trilogy #3   \n",
       "3           3        Shadow and Bone  The Shadow and Bone Trilogy #1   \n",
       "4           4  An Ember in the Ashes        An Ember in the Ashes #1   \n",
       "\n",
       "     bookAuthors  ratingValue  ratingCount  reviewCount  \\\n",
       "0  Jasmine  Rose         4.38           69           13   \n",
       "1     Cassia Leo         4.02         6244          903   \n",
       "2  Leigh Bardugo         4.09       158624        19396   \n",
       "3  Leigh Bardugo         3.99       313190        32760   \n",
       "4    Sabaa Tahir         4.27       184959        23147   \n",
       "\n",
       "                                         Plot_Values  \\\n",
       "0         {5: '47', 4: '7', 3: '11', 2: '2', 1: '2'}   \n",
       "1  {5: '2297', 4: '2320', 3: '1181', 2: '345', 1:...   \n",
       "2  {5: '62107', 4: '59607', 3: '27962', 2: '6810'...   \n",
       "3  {5: '108466', 4: '121286', 3: '62748', 2: '146...   \n",
       "4  {5: '91602', 4: '63735', 3: '21164', 2: '5487'...   \n",
       "\n",
       "                                                Plot  NumberofPages  \\\n",
       "0  ❝�� my heart has become a planetand you are th...            NaN   \n",
       "1  ♥️ Three fateful encounters....♥️ Two heart-br...          400.0   \n",
       "2  ▶ \\nAlternative Cover Edition #1\\nThe capital ...          422.0   \n",
       "3  ▶ \\nAlternative Cover Edition #1\\nSurrounded b...          358.0   \n",
       "4  ▶ \\nAlternative Cover Edition #1\\nLaia is a sl...          446.0   \n",
       "\n",
       "      Publishing_Date                                         Characters  \\\n",
       "0                2014                                                NaN   \n",
       "1  February 28th 2014      ['Mikki Gladstone', 'William \"Crush\" Slayer']   \n",
       "2      June 17th 2014  ['Alina Starkov', 'Malyen Oretsev', 'Darkling'...   \n",
       "3       June 5th 2012  ['Alina Starkov', 'Malyen Oretsev', 'Darkling'...   \n",
       "4   February 9th 2016  ['Laia', 'Elias Veturius', 'Helene Aquilla', '...   \n",
       "\n",
       "                     Setting  \\\n",
       "0                        NaN   \n",
       "1  ['Boston, Massachusetts']   \n",
       "2                 ['Ravka ']   \n",
       "3                 ['Ravka ']   \n",
       "4                        NaN   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://www.goodreads.com/book/show/23279048-mars  \n",
       "1  https://www.goodreads.com/book/show/29539518-b...  \n",
       "2  https://www.goodreads.com/book/show/14061957-r...  \n",
       "3  https://www.goodreads.com/book/show/10194157-s...  \n",
       "4  https://www.goodreads.com/book/show/27774758-a...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"Dataset.tsv\",sep='\\t')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "### Question 2\n",
    "*Now, we want to create two different **Search Engines** that, given as input a query, return the books that match the query*.\n",
    "\n",
    "First, you must pre-process all the information collected for each book by\n",
    "\n",
    "* Removing stopwords\n",
    "* Removing punctuation\n",
    "* Stemming\n",
    "\n",
    "Anything else you think it's needed\n",
    "For this purpose, you can use the **nltk library**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.1 Conjunctive query\n",
    "*For the first version of the search engine, we narrow our interest on the Plot of each document. It means that you will evaluate queries only with respect to the book's plot.*<br>\n",
    "\n",
    "##### <br>2.1.1 Create your index!\n",
    "*Before building the index,<br>*\n",
    "<br>\n",
    "Create a file named **vocabulary**, in the format you prefer, that maps each word to an integer (term_id).<br>\n",
    "Then, the first brick of your homework is to create the **Inverted Index**. It will be a dictionary of this format:<br>\n",
    "<br>\n",
    "{<br>\n",
    "term_id_1:[document_1, document_2, document_4],<br>\n",
    "term_id_2:[document_1, document_3, document_5, document_6],<br>\n",
    "...}<br>\n",
    "where **document_i** is the id of a document that contains the word.<br>\n",
    "<br>\n",
    "Hint: Since you do not want to compute the inverted index every time you use the **Search Engine**, it is worth to think to store it in a separate file and load it in memory when needed.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <br><br> 2.1.2 Execute the query\n",
    "Given a query, that you let the user enter:<br>\n",
    "<br>\n",
    "**survival games<br>**<br>\n",
    "the Search Engine is supposed to return a list of documents.<br>\n",
    "<br>\n",
    "What documents do we want?<br>\n",
    "Since we are dealing with conjunctive queries (AND), each of the returned documents should contain all the words in the query. The final output of the query must return, if present, the following information for each of the selected documents:<br>\n",
    "<br>\n",
    "* bookTitle\n",
    "* Plot\n",
    "* Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### 2.2 Conjunctive query & Ranking score\n",
    "*For the second search engine, given a query, we want to get the top-k (the choice of k it's up to you!) documents related to the query. In particular:*\n",
    "\n",
    "<br>Find all the documents that contains all the words in the query.\n",
    "Sort them by their similarity with the query\n",
    "<br>Return in output k documents, or all the documents with non-zero similarity with the query when the results are less than k. You must use a heap data structure (you can use **Python libraries**) for maintaining the **top-k documents**.<br>\n",
    "*To solve this task, you will have to use the tfIdf score, and the Cosine similarity. The fielf to consider it is still the plot. Let's see how.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "##### 2.2.1 Inverted index\n",
    "**Your second Inverted Index must be of this format:**\n",
    "\n",
    "{<br>\n",
    "**term_id_1**:[(document1, tfIdf_{term,document1}), (document2, tfIdf_{term,document2}), (document4, tfIdf_{term,document4}), ...]<br>\n",
    "...}<br><br>\n",
    "Practically, for each word you want the list of documents in which it is contained in, and the relative **tfIdf score**.\n",
    "\n",
    "**Tip**: tfIdf values are invariant with respect to the query, for this reason you can precalculate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Execute the query\n",
    "*In this new setting, given a query you get the right set of documents (i.e., those containing all the words in the query) and sort them according to their **similairty** to the query. For this purpose, as scoring function we will use the Cosine Similarity with respect to the tfIdf representations of the documents.*\n",
    "<br><br>\n",
    "*Given a query, that you let the user enter:*\n",
    "<br><br>\n",
    "##### survival games<br>\n",
    "the search engine is supposed to return a list of documents, ranked by their Cosine Similarity with respect to the query entered in input.\n",
    "<br><br>\n",
    "**More precisely, the output must contain:**\n",
    "<br><br>\n",
    "* bookTitle\n",
    "* Plot\n",
    "* Url\n",
    "* The similarity score of the documents with respect to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Execute the query\n",
    "In this new setting, given a query you get the right set of documents (i.e., those containing all the words in the query) and sort them according to their similairty to the query. For this purpose, as scoring function we will use the Cosine Similarity with respect to the tfIdf representations of the documents. <br>\n",
    "<br>\n",
    "Given a query, that you let the user enter:\n",
    "<br>\n",
    "**survival games**<br>\n",
    "the search engine is supposed to return a list of documents, ranked by their Cosine Similarity with respect to the query entered in input.\n",
    "<br><br><br>\n",
    "More precisely, the output must contain:\n",
    "\n",
    "* bookTitle\n",
    "* Plot\n",
    "* Url\n",
    "* The similarity score of the documents with respect to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the Query: Survival games\n",
      "Query cleaned:  ['survival', 'game']\n"
     ]
    }
   ],
   "source": [
    "#Function \"read_data\" will get Filtered Dataset, Dictionary of Vocabulary and Inderted Index Dictionary as ID \n",
    "dataset,vocabulary,ID = read_data()\n",
    "dataset.head()\n",
    "query = input(\"Please enter the Query: \")\n",
    "query = clean_info1(query.split(' '))\n",
    "query = query.split(' ')\n",
    "\n",
    "#Sample Query: * Survival games hunger\n",
    "print(\"Query cleaned: \",query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17683</th>\n",
       "      <td>Warcross</td>\n",
       "      <td>For the millions who log in every day, Warcros...</td>\n",
       "      <td>https://www.goodreads.com/book/show/41014903-w...</td>\n",
       "      <td>0.126649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24202</th>\n",
       "      <td>Blood Awakening</td>\n",
       "      <td>A dangerous game of life, blood, and survival…...</td>\n",
       "      <td>https://www.goodreads.com/book/show/11030236-b...</td>\n",
       "      <td>0.067923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10305</th>\n",
       "      <td>Code Name Verity</td>\n",
       "      <td>Oct. 11th, 1943 - A British spy plane crashes ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/11925514-c...</td>\n",
       "      <td>0.067923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6116</th>\n",
       "      <td>U-10</td>\n",
       "      <td>The U-10 is a survival kit you might not survi...</td>\n",
       "      <td>https://www.goodreads.com/book/show/51106657-u-10</td>\n",
       "      <td>0.067923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5513</th>\n",
       "      <td>Inside the Maze Runner: The Guide to the Glade</td>\n",
       "      <td>The first book in James Dashner’s New York Tim...</td>\n",
       "      <td>https://www.goodreads.com/book/show/21965351-i...</td>\n",
       "      <td>0.035954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            bookTitle  \\\n",
       "17683                                        Warcross   \n",
       "24202                                 Blood Awakening   \n",
       "10305                                Code Name Verity   \n",
       "6116                                             U-10   \n",
       "5513   Inside the Maze Runner: The Guide to the Glade   \n",
       "\n",
       "                                                    Plot  \\\n",
       "17683  For the millions who log in every day, Warcros...   \n",
       "24202  A dangerous game of life, blood, and survival…...   \n",
       "10305  Oct. 11th, 1943 - A British spy plane crashes ...   \n",
       "6116   The U-10 is a survival kit you might not survi...   \n",
       "5513   The first book in James Dashner’s New York Tim...   \n",
       "\n",
       "                                                     Url  Similarity  \n",
       "17683  https://www.goodreads.com/book/show/41014903-w...    0.126649  \n",
       "24202  https://www.goodreads.com/book/show/11030236-b...    0.067923  \n",
       "10305  https://www.goodreads.com/book/show/11925514-c...    0.067923  \n",
       "6116   https://www.goodreads.com/book/show/51106657-u-10    0.067923  \n",
       "5513   https://www.goodreads.com/book/show/21965351-i...    0.035954  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "founddata = executeQuery(query,vocabulary,ID,dataset)\n",
    "founddata.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <br><br> Question 3 \n",
    "##### Define a new score!<br>\n",
    "Now it's your turn. Build a new metric to rank books based on the **queries** of their users.<br>\n",
    "<br>\n",
    "In this scenario, a single user can give in input more information than the single textual query, so you need to take into account all this information, and **think a creative and logical** way on how to answer at user's requests.<br>\n",
    "<br>\n",
    "Practically:<br>\n",
    "<br>\n",
    "The user will enter you a text query. As a starting point, get the **query-related** documents by exploiting the search engine of Step 3.1.<br>\n",
    "<br>\n",
    "Once you have the documents, you need to sort them according to your new score. In this step you won't have anymore to take into account just the plot of the documents, you must use the remaining variables in your dataset (or new possible variables that you can create from the existing ones...). You must use a heap data structure ***(you can use Python libraries)*** for maintaining the top-k documents.<br>\n",
    "<br>\n",
    "**Q**: *How to sort them? A: Allow the user to specify more information, that you find in the documents, and define a new metric that ranks the results based on the new request.*<br>\n",
    "<br>\n",
    "**N.B**: You have to define a ***scoring function***, not a filter!<br>\n",
    "<br>\n",
    "The output, must contain:<br>\n",
    "<br>\n",
    "* bookTitle\n",
    "* Plot\n",
    "* Url\n",
    "* The similarity score of the documents with respect to the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <br><br> Question 5 \n",
    "##### Algorithmic Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>You are given a string written in english capital letters, for example **S=\"CADFECEILGJHABNOPSTIRYOEABILCNR.\"** You are asked to find the maximum length of a subsequence of characters that is in alfabetical order. For example, here a subsequence of characters in alphabetical order is the **\"ACEGJSTY\": \"CADFECEILGJHABNOFPSTIRYOEABILCNR.\"** Among all the possible such sequences, you are asked to find the one that is the longest.<br>\n",
    "<br>\n",
    "Define as ***X[i] = \"the length of the longest sequence of characters in alphabetical order that terminates at the i-th character\".*** One can prove that\n",
    "<br><br>\n",
    "**X[i] = 1 + max{X[j]; j = 0, ..., i-1, such that S[j]<S[i]}<br>**\n",
    "<br>\n",
    "**X[i] = 1, if there does not exist such a j.<br>**\n",
    "<br>\n",
    "Write a recursive program that, given a string, computes the length of the subsequence of maximum length that is in alphabetical order. Try some examples. Are the examples of short strings correct? Can you find examples that your algorithm does not terminate in reasonable time?<br><br>\n",
    "Show that the running time of the algorithm is exponential.<br><br>\n",
    "Write a program that computes the length of the subsequence of maximum length, using dynamic programming.<br>\n",
    "What is its runtime complexity?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
