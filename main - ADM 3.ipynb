{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1. Get the list of books\n",
    "We start from the list of books to include in your corpus of documents. In particular, we focus on the best books ever list. From this list we want to collect the url associated to each book in the list. As you realize, the list is long and splitted in many pages. We ask you to retrieve only the urls of the books listed in the first 300 pages.**\n",
    "\n",
    "*The output of this step is a .txt file whose single line corresponds to a book's url.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bsp\n",
    "from selenium import webdriver #Programmatic way to use Browser\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "import time\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findUrls(href,driver):\n",
    "    driver.get(href)\n",
    "    time.sleep(1)\n",
    "    soup = bsp(driver.page_source,features='lxml')\n",
    "    list_urls = soup.find_all('a',{\"class\":\"bookTitle\"},itemprop='url')\n",
    "    urls = []\n",
    "    for url in list_urls:\n",
    "        urls.append(url.get(\"href\"))\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBookUrls(href,driver,pageRange):\n",
    "    Urls_Dic = {}\n",
    "    #Urls_List = []\n",
    "    change = 1\n",
    "    for pageNumber in range (1,pageRange+1):\n",
    "            if change != len(str(pageNumber)):\n",
    "                change = len(str(pageNumber))\n",
    "                href = href[:-(len(str(pageNumber))-1)] + str(pageNumber)\n",
    "                Urls_Dic[\"page\"+str(pageNumber)] = findUrls(href,driver)\n",
    "                #Urls_List.append(findUrls(href,driver))\n",
    "                print(href)\n",
    "                continue\n",
    "            href = href[:-len(str(pageNumber))] + str(pageNumber)\n",
    "            Urls_Dic[\"page\"+str(pageNumber)] = findUrls(href,driver)\n",
    "            #Urls_List.append(findUrls(href,driver))\n",
    "            print(href)\n",
    "    return Urls_Dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1\"\n",
    "Urls_Dic = findBookUrls(href,driver,300)\n",
    "DictionaryPKL = open(r\"C:\\Users\\Zain\\Desktop\\ROMA\\Sapienza\\ADM\\HW3\\300PagesDic.pkl\",\"wb\")\n",
    "pickle.dump(Urls_Dic,DictionaryPKL)\n",
    "DictionaryPKL.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\Zain\\Desktop\\ROMA\\Sapienza\\ADM\\HW3\\300PagesDic.pkl', 'rb') as handle:\n",
    "    pagesDict = pickle.load(handle)\n",
    "\n",
    "with open(r'C:\\Users\\Zain\\Desktop\\ROMA\\Sapienza\\ADM\\HW3\\300Pages_UrlsList.txt','w+') as File:\n",
    "    FlatList = [ item for elem in list(pagesDict.values()) for item in elem]\n",
    "    File.write('\\n'.join(FlatList))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2. Crawl books\n",
    "Once you get all the urls in the first 300 pages of the list, you:**\n",
    "\n",
    "Download the html corresponding to each of the collected urls.\n",
    "**After you collect a single page, immediatly save its html in a file. In this way, if your program stops, for any reason, you will not loose the data collected up to the stopping point. More details in Important (2).**\n",
    "Organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the books in page 1, page 2, ... of the list of books.\n",
    "**Important\n",
    "Due to the large amount of pages you need to download, we give you the following tipds that help you to speed up some time-consuming operations.**\n",
    "\n",
    "**[Save time downloading files]** You are asked to crawl tons of pages, and this will take a lot of time. To speed up the operation, we suggest you to work in parallel with your group's colleagues. In particular, using the same code, each component of the group can be in charge of downloading a subset of pages (e.g., the first 100). PAY ATTENTION: Once obtained all the pages, merge your results in a unique dataset. In fact, the search engine must look up for results in the whole set of documents.**\n",
    "\n",
    "**[Save your data]** It is not nice to restart a crawling procedure, given its runtime. For this reason, it is extremely important that for every time you crawl a page, you must save it with the name article_i.html, where i corresponds to the number of articles you have already downloaded. In such way, if something goes bad, you can restart your crawling procedure from the i+1-th document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your Start and End Range of Pages\n",
    "#Nicole: You have to just Run it noting else\n",
    "#And Giacomo you just have to Put Start and End Range\n",
    "Start = 101\n",
    "End = 200\n",
    "\n",
    "#Change the Path here\n",
    "FilePath = r\"Users\\Zain\\Desktop\\ROMA\\Sapienza\\ADM\\Test\"\n",
    "\n",
    "\n",
    " \"\"\"              Don't Write below this line ;)   \"\"\"\n",
    "    \n",
    "with open(r'C:\\{}\\300PagesDic.pkl'.format(FilePath), 'rb') as handle:\n",
    "    pagesDict = pickle.load(handle)\n",
    "\n",
    "pagesDict = dict(list(pagesDict.items())[Start-1:End]) \n",
    "    \n",
    "dir = os.path.join(\"C:\\\\\",FilePath,\"Best Book Ever\")\n",
    "if not os.path.exists(dir):\n",
    "    os.mkdir(dir)     \n",
    "    \n",
    "file_location = r\"{}\\Best Book Ever\".format(FilePath)\n",
    "for pageFolder in pagesDict:\n",
    "    dir = os.path.join(\"C:\\\\\",file_location,pageFolder)\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "    Urls_L = pagesDict[pageFolder]\n",
    "    count = 0\n",
    "        \n",
    "    for urls in Urls_L:\n",
    "        href = \"https://www.goodreads.com/\" + urls\n",
    "        driver.get(href)\n",
    "        HTMLFileName = pageFolder+\"_\"+str(count)\n",
    "        with open(r'C:\\{}\\Best Book Ever\\{}\\{}.html'.format(FilePath,pageFolder,HTMLFileName),'w+', encoding=\"utf-8\") as Page:\n",
    "            Page.write(driver.page_source)\n",
    "        print(pageFolder+\" File:\"+str(count))\n",
    "        count = count + 1\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(yourDic['page101'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
